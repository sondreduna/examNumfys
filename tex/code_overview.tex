The source code for this project is structured into separate files for each sub-problem, i.e. \lstinline|prob_2A.py|, \lstinline|prob_2B.py| etc. Additionally, there is a file for plotting for each sub-problem. I have also included a file called \lstinline|utils.py| which contains some useful utility functions used throughout the exercises. For problem $2$A I have used the same ODE-solver as the one I made for exercise $2$. This can be found in \lstinline|ode.py|.
\subsection{Remarks on performance}
In many parts I gained a lot of performance by compiling the code with \lstinline|numba|, as usual. For example, if we solve the stochastic SIR equations with the parameters given in problem 2Ba, and time step $\Delta t = 0.005$ the performance is as follows:
\begin{lstlisting}
# without numba
%timeit T,v  = stochSIR(v_0,tN,dt,beta,tau)
\end{lstlisting}
\texttt{\small388 ms ± 5.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)}

To contrast, If we compile both the solver function and the stepper function with \lstinline|numba| the code runs approximately $50$ times as fast:
\begin{lstlisting}
# with numba
%timeit T,v  = stochSIR(v_0,tN,dt,beta,tau)
\end{lstlisting}
\texttt{\small7.46 ms ± 107 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
}

My solution for solving the commuter model worked reasonably well in problem 2D, but when I reached problem 2E I found it to be \textit{very} memory consuming, as I store the number of people in each state for each group of people for each time step, which amounts to an array of size
$$
	\#\text{time steps}\times\#\text{towns} \times\#\text{towns} \times 5.
$$ 
For the population structure in problem 2Eb this yields an array of approximately $4.7$ GiB when using $1000$ time steps. To avoid these memory issues I modify the solver to only save the number of people in each state for each group of people for the current and previous time step. This allows for running short time steps without any memory issues, which, importantly,  makes it possible to run the $10$ simulations in parallel. When naïvely using the solver from problem 2D on the population matrix in problem 2Eb with a step length of $0.1$ the magic function \lstinline|%memit| from IPython produces:

\texttt{\small peak memory: 10686.46 MiB, increment: 8707.98 MiB}.

From profiling the code used in problem 2Eb I found that the most time consuming part was the stepping function which draws random numbers for each entry in the matrix. I initially tried to draw these random numbers in a vectorised fashion, but found I it exceedingly hard to understand what the functions were doing when providing multidimensional inputs, and I started to get issues with people disappearing from the population. However, I found major improvements just by modifying the line where I step forward using the function \lstinline|SEIIaR_commuter_step()| to the following:
\begin{lstlisting}[language=Python]
if M[l,k] == 0:
    X[l,k,:] = X_[l,k,:]
else:
    X[l,k,:] = SEIIaR_commuter_step(X_[l,k,:],Pse[k],Pei,Peia,Pir,Piar)
\end{lstlisting} 

This is particularly useful in the case of problem 2Eb and c, as the population matrix is very sparse (see figure \ref{fig:matrices} for an illustration of the population structure). Testing with and without this change, with a step length of $0.1$ yields the following results:
\begin{lstlisting}
# Solution without skipping empty entries
dt = 0.1
%time T, I = SEIIaR_commuter_greedy(M,X_0,tN,dt)
\end{lstlisting}
\texttt{\small CPU times: user 1min 17s, sys: 230 ms, total: 1min 17s
Wall time: 1min 17s}
\begin{lstlisting}
# Solution _with_ skipping empty entries
dt = 0.1
%time T, I = SEIIaR_commuter_greedy(M,X_0,tN,dt)
\end{lstlisting}
\texttt{\small CPU times: user 22.4 s, sys: 58.5 ms, total: 22.5 s
Wall time: 22.4 s}

A further slight improvement is obtained by only calculating the number of people in each town during daytime and the night once. This brings the runtime slightly lower:
\texttt{\small CPU times: user 18.2 s, sys: 46.8 ms, total: 18.3 s
Wall time: 18.2 s
}

Initially I did this calculation for every iteration, but realised quickly that this was not necessary. I have experienced through this course that the workflow of writing inefficient but nontheless understandable code to begin with and subsequently trying to optimise it has been way more efficient than trying to find the most optimal solution right away. I find that the example of making this solver faster illustrates this workflow quite nicely.
 

%Although not a central part of the solver at all, I found a very neat way of expressing and calculating the inital state of a system, given the population matrix. To initialise all people in the suscpetible state, one use the tensorproduct:
%$$
%	\mathbf{X}_0 = \mathbf{M} \odot \mathbf{X},
%$$
%where $\mathbf{X} = [1,0,0,0,0]^T$ and $\mathbf{M}$ is the $m\times m$ dimensional population matrix. This yields the initial state $\mathbf{X}_0$ for all groups of people, i.e. the $m\times m\times 5$ matrix. This can be calulcated in \lstinline|numpy| by using \lstinline|np.tensordot(M,X,axes = 0)|. Creating the initial state in a "brute force" fashion is seen to be much slower: 

